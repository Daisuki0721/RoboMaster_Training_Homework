# Q_learning

这个我曾经接触过，对我难度也不高

核心是Q表的生成，而Q值的迭代公式如下：
$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \space \max_{a'} Q(s', a') - Q(s, a) \right]
$$
时序差分误差和学习率是这个公式的关键。$ \alpha $ 表示学习率，控制新学到的知识覆盖经验的程度
$ R(s, a) + \gamma \space \max_{a'} Q(s', a') - Q(s, a) $
表示TD误差，是目标与当前估计的距离

利用正态分布随即决定当前的下一步是随机探索还是依赖先前的经验（即Q表中的最大值）

在训练中，随着迭代次数的增加缓慢降低探索的概率，使智能体越来越倾向于依照已有的经验走下一步

这样就完成了对人类试错学习法的基本模拟过程

使用MatPlotlib绘制学习曲线，这个奖惩机制设计的还挺恰当的，收敛很快（

一个文件搞定了就不分头文件和源文件了吧）
